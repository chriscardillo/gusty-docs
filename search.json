[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "gusty",
    "section": "",
    "text": "Preface\nOrchestration, or the routine scheduling and exection of dependent tasks, is a core component of modern data work. Orchestration continues to reach more and more data workers - it was originally a focus for data engineers, but it now permeates the work of data analysts, analytics engineers, data scientists, and machine learning engineers. The easier it is for any class of data worker to orchestrate their code, the easier it is for any member of an organization to derive value from the outputs of that code."
  },
  {
    "objectID": "index.html#flavors-of-orchestration-code",
    "href": "index.html#flavors-of-orchestration-code",
    "title": "gusty",
    "section": "Flavors of Orchestration Code",
    "text": "Flavors of Orchestration Code\nOrchestration with Python is a vast and opinionated landscape, but there are three clear flavors of orchestration to have emerged over time:\n\nObject-oriented orchestration, where tasks are objects and dependencies between tasks are handled with methods. Airflow’s classic style is a good example of object-oriented orchestration.\nDecorative orchestration, where tasks are functions and decorators are used to configure the tasks. Airflow’s taskflow API and Dagster’s entire API are good examples of decorative orchestation.\n“File as a Task” (FaaT) orchestration, where tasks are files. Tools like Mage, dbt, and Orchest exemplify FaaT orchestration."
  },
  {
    "objectID": "index.html#what-is-gusty",
    "href": "index.html#what-is-gusty",
    "title": "gusty",
    "section": "What is gusty?",
    "text": "What is gusty?\ngusty is a FaaT framework for Airflow, the absolute standard for orchestrators today. While other orchestrators natively support FaaT, Airflow is a Top-Level Apache Project with sustained development, a gigantic ecosystem of provider packages, and is offered as a hosted service by major public clouds and other Airflow-focused companies. If you are reading this, you’re probably already familiar with - or using - Airflow.\ngusty exists to make FaaT orchestration fun and easy using Airflow, allowing for FaaT DAGs to be incorporated in existing Airflow projects without any need to change existing work or Airflow code."
  },
  {
    "objectID": "gusty-basics.html#gusty-dag-structure",
    "href": "gusty-basics.html#gusty-dag-structure",
    "title": "1  gusty Basics",
    "section": "1.1 gusty DAG Structure",
    "text": "1.1 gusty DAG Structure\nA gusty DAG lives inside of your Airflow DAGs folder (by default $AIRFLOW_HOME/dags), and is comprised of a few core elements:\n\nTask Definition Files - Each file hold specifications for a given task. In the example below, hi.py, hey.sql, and hello.yml are our Task Definition Files. These Task Definition Files are all stored inside our hello_dag folder.\nMETADATA.yml - This optional file contains any argument that could be passed to Airflow’s DAG object, as well as some optional gusty-specifc argument. In the example below, METADATA.yml is stored inside of our hello_dag folder, alongside the Task Definition Files.\nDAG File - The file that turns a gusty DAG folder into an Airflow DAG. It’s more or less like any other Airflow DAG file, and it will contain gusty’s create_dag function. In the example below, hello_dag.py is our DAG Generation File. The DAG Generation File does not need to be named identically to the DAG folder.\n\n$AIRFLOW_HOME/dags/\n│\n├── hello_dag/\n│   ├── METADATA.yml\n│   ├── hi.py\n│   ├── hey.sql\n│   └── hello.yml\n│\n└── hello_dag.py\nIn the event you wanted to create a second gusty DAG, you can just repeat this pattern. For example, if we wanted to add goodbye_dag:\n$AIRFLOW_HOME/dags/\n│\n├── goodbye_dag/\n│   ├── METADATA.yml\n│   ├── bye.py\n│   ├── later.sql\n│   └── goodbye.yml\n|\n├── hello_dag/\n│   ├── METADATA.yml\n│   ├── hi.py\n│   ├── hey.sql\n│   └── hello.yml\n│\n├── goodbye_dag.py\n└── hello_dag.py"
  },
  {
    "objectID": "gusty-basics.html#task-definition-files",
    "href": "gusty-basics.html#task-definition-files",
    "title": "1  gusty Basics",
    "section": "1.2 Task Definition Files",
    "text": "1.2 Task Definition Files\nThe three primary file types used for Task Definition Files are Python, SQL, and YAML. gusty supports other file types, but these three are the most commonly used. The general pattern for Task Definition files is that they contain:\n\nFrontmatter - YAML which carries the specification and parameterization for the task. This can include which Airflow (or custom) operator to use, any keyword arguments to be passed to that operator, and any task dependencies the given task may have.\nBody - The primary contents of the task. For example, the Body of a SQL file is the SQL statement which will be executed; the body of a Python file can be the python_callable that will be ran by the operator. For YAML files, there is no Body because the whole Task Definition File is YAML.\n\ngusty will pass any argument that can be passed to the operator specified (as well as any BaseOperator arguments) to the operator. The specified operator should be a full path to that operator.\nThe file name of each Task Definition File will become the name of the Airflow task.\nLet’s explore these different file types by looking at the contents of these Task Definition Files in hello_dag.\n\nYAML Files with hello.yml\nHere are the contents of our hello.yml file:\noperator: airflow.operators.bash.BashOperator\nbash_command: echo hello\nBecause the entire file is YAML, there is no separation of Frontmatter and Body.\nThe resulting task would contain a BashOperator with the task id hello.\n\n\nSQL Files with hey.sql\nHere are the contents of our hey.sql file:\n---\noperator: airflow.providers.sqlite.operators.sqlite.SqliteOperator\n---\n\nSELECT 'hey'\nThe resulting task would contain a SqliteOperator with the task id hey.\nThe Frontmatter of our SQL file is encased in a set of triple dashes (---). The Body of the file is everything below the second set of triple dashes. For SQL files, the Body of the file is passed to the sql argument of the underlying operator. In this case, SELECT 'hey' would be passed to the sql argument.\n\n\nPython Files with hi.py\nHere are the contents of our hi.py file:\n# ---\n# python_callable: say_hi\n# ---\n\ndef say_hi():\n  phrase = \"hi\"\n  print(phrase)\n  return phrase\nThe resulting task would contain a PythonOperator with the task id hi.\nThe Frontmatter of our Python file is also encased in a set of triple dashes (---), but you will also note that the entirety of the Frontmatter, including the triple dashes, are prefixed by comment hashes (#).\nBy default, gusty will specify specify Airflow’s PythonOperator as the operator, if no operator argument is provided. As with any Task Definition File, you can specify whatever operator is available to you in your Airflow environment, so you could just as easily add operator: airflow.operators.python.PythonVirtualenvOperator to this Frontmatter to use the PythonVirtualenvOperator instead of the PythonOperator.\nWhen a python_callable is specified in the Frontmatter of a Python file, gusty will search the Body of the Python file for a function with the name specified in the Frontmatter’s python_callable argument. For the best results with Python files, it’s recommended that you put all of the Body contents in a named function, as illustrated above."
  }
]